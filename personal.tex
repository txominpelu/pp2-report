


\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{graphicx}

\title{NoWork\\ Personal Report}
\author{Inigo Mediavilla\\[2em]}
\date\today

\begin{document}
\maketitle

%% Index
%% - Introduction to Rewriting system (what it is, what is useful for)
%% - Justification for the project (what have we gained from doing,
%% how could a user benefit from it)
%%
%% - Global vision of the project (short description of how to
%% describe a language with the system, describing every one of the concepts)
%%
%% - Subsystems (a short description of every part
%% of the project (parsing, typechecking
%% - How does the system work? Explain how the rewriting of a system
%% is implemented by giving a term and explaining what is the term,
%% what is the pattern, what is the metavariable of placeholder
%% -
%% - Problem with rewriting (what subtree should I replace first -
%% show with an example why it matters because it affects the result)
%% - Initial Naive version of rewriting: (1. without fixpoint (Problems)
%% 2. Bottom-up and Top-down 3. Higher flexibility Strategies)
%%
%% - Conclusion

\section{Introduction to rewriting systems}

Rewriting theories are systems that are defined as a set of transformations called
rewriting rules. These rewriting rules are the core of the theory and
they define its computational power as well as its expressivity and other properties.

These systems have applications that are not restricted to the well known general 
paradigm programming languages for computers. Systems based on this kind of rules have been used for
the analysis of bussiness interactions or for the formalization of
some logics, for example the Brouwer-Heyting-Kolmogorov interpretation has shown that
predicate logic can be expressed as a set of rewriting rules.

One of the reasons why this kind of systems are so widely used is that
they have been well studied, they are well understood and a
wide range of tools exist to prove properties on them.

\section{Justification}

Despite the multiple applications of these systems and all the
mathematical tools available to prove properties on them building a
new rewriting theory or tuning an existing one for experimentation is
still a really tedious task. The implementation of this kind of
system from scratch may require building a parser for the theory, a
type checker, and all the code to find matches for a rule and
transforming the existing term when matches are found. Most of these
steps require a great amount of work even though they're relatively
simple. Besides a great amount of this work could be generalized since
the differences between two of these systems are restricted to the
definition of the rules.

This is the purpose of Nowork, to provide the generic part in the
implementation of rewriting theories allowing users to define new
theories by only providing the rules that define them.

%% It also allows to prove some properties on them and further work
%% on nowork could also facilitate the automatic verification of
%% properties for these systems.


\section{ NoWork - Rewriting System }

NoWork users can define their own rewriting system without having to
write their own parser, typechecker and without having to code the
reduction rules of the interpreter. To do that, NoWork offers a language 
to describe the values, operations, types and rewriting rules for a theory.

Rules are defined as a pattern and a transformation. The pattern
describes the model that NoWork will try to find in a given term. The 
transformation is the substitution that will be applied on the tree when
a match for the model is found.

\begin{figure}[!h]
\begin{center}
\begin{verbatim}

rule = pattern + transformation

rule [beta] :
  App(Lambda(?x, ?T), ?U)  => Subst(?x, ?T, ?U)

pattern = App(Lambda(?x, ?T), ?U) 
transformation = Subst(?x, ?T, ?U)

\end{verbatim}
\end{center}
\caption{Structure of a pattern}
\end{figure}

NoWork allows a simple syntax for defining Constants.

\begin{figure}[!h]
\begin{verbatim}

constant Nil : forall(A).List<A>
\end{verbatim}
\caption{Definition of a constant}
\end{figure}


It also allows to define Operators. Since all the
reduction in NoWork is specified in the rewriting rules, operators 
are not like the functions we know from functional programming languages. 
Operators just give the name and the type of a reduction. Usually in the
definition of a theory every operator will have a rewriting rule
and if the language is well formed the transformation will conform to
the typing defined in the operator.


\begin{figure}[!h]
\begin{verbatim}

operator Cons : forall(A).A * List<A> -> List<A>

\end{verbatim}
\caption{Definition of an operator}
\end{figure}

Kinds allow to define the constant types and also the polymorphic types when
their definition depends on a type variable.

\begin{figure}[!h]
\begin{verbatim}

kind Bool : type # Constant type
kind List : type -> type # Polymorphic type
\end{verbatim}
\caption{Definition of kinds}
\end{figure}

Therefore, the rewriting rules provide the semantics of the language
while the type annotations provided in the constants, the operators and
the kinds allow the user to define the typing rules. All together
this language helps the user concisely define a programming language
declaratively without having to implement a typechecker and
an interpreter.

\section{Global vision of the system}

We can consider that code written in NoWork belongs to one of two
different categories:

Definitions of theories. Written in NoWork's language. Contain the
constants, operators, kinds and rules as they have been described
previously. This definitions must conform to the grammar and type
checking rules of NoWork's language to avoid the definition of
malformed theory.

Theory Terms. Once a theory has been defined with NoWork, terms of
that theory can be written as trees. This trees should conform to the
typing rules that have been specified by the theory and that are
independent of the rules for defining the theory.

In addition to the two previous categories that could be considered two independent 
languages, NoWork provides a mini-language with a list of commands that are parsed by
the toplevel and that allow the user to interact with the
program. In the following section we will see a generalization of the process that is
executed when NoWork receives a command with a description of the most important steps.

\subsection{Process}

The process by which NoWork can define new rewriting systems and
evaluate terms can be reduced to the following steps:

\begin{figure}[!h]
\begin{verbatim}
Parsing + Type Checking  + Rewriting
Rewriting = pattern matching + transformation
\end{verbatim}
\caption{ Phases of the evaluation of a command }
\end{figure}

Parsing: This step reads a string and converts it to an AST that
can contain a definition of a theory or an interaction with a term
of the theory.

Typechecking: If the command is a definition of a theory, NoWork's
language typing rules are verified. Otherwise the command is an evaluation 
and the term that will be evaluated needs to be typed checked against the definition of
the theory contained in the environment.

Rewriting: This phase occurs only when the command is not a definition. 
In this phase the term is explored looking for matches of the patterns described in the
rewriting rules. When a match is found the transformation is applied to the part of the term where the match
was found.

\section{Rewriting}

Rules are defined as patterns with placeholders. 
When a match is found the placeholders act as variables whose
values are given by the content of the match. Those variables will be used to provide
values to the result of the transformation. In a simple version of pattern matching
called linear pattern matching they act exactly like capture groups in regular
expresions. NoWork has implemented a more complex version of
pattern matching called non-linear pattern matching where the same
placeholder can appear many times in the same pattern.

For example if we take again the initial example we can see the
pattern, the placeholder and the transformation in the beta reduction rule of 
lambda calculus.

\begin{figure}
\begin{verbatim}
App(Lambda(?x, ?T), ?U) => Subst(...)

Pattern = App(Lambda(?x, ?T), ?U)
Transformation = Subst(?x, ?T, ?U)
Placeholders = {x, T, U}

When applied to:

App(Lambda(a, a), b)
Environment = { x = a, T = a, U = b }

\end{verbatim}
\caption{Elements of a pattern}
\end{figure}


% TODO: Explain that rewriting takes a node and it asks the pattern matcher if the node conforms to the rule
% if it doesn't it will navigate the tree
During the rewriting process the rewriting mechanism takes a rule and 
asks the pattern matcher to look for matches of the rule's pattern in the term. 
If the pattern matcher finds a match, it
returns the matches with the values for the placeholders and
the rewriting process only has to fill the holes that exist in the rule's
transformation with the values of the environment generated from the matching. If the
pattern matcher doesn't find a match for the rule, then the rewriting process restarts
the process with another rule.

%% Rewriting Algorithm in pseudocode


\begin{figure}
\begin{verbatim}
Environment = { x = a, y = a, z = b }

Subst (x, y, z ) in Environment => Subst (a, a, b) 

\end{verbatim}
\caption{Rewriting last step}
\end{figure}

However this really simple vision of rewriting hides a small subtlety
that concerns the way the term is traversed during rewriting
and how the transformations are applied. The initial problem is that
a term can have many occurrences of the same match and depending on
how the tree is traversed the result of applying the same set of rules
may be different.


\begin{figure}[!h]
\includegraphics[scale=0.4]{multiple-reductions.png}

\caption{Term with different reductions}
\label{multiple-reductions}
\end{figure}

In figure~\ref{multiple-reductions} we can see how there are two possible reductions that are marked with
a darker background. Either the reduction can happen at the root of
the tree or the reduction can happen in the application that appears
on the right child of the root. In this case if we keep
reducing until we get to an irreducible term the result will be the
same independently of which one of those reductions we apply
first. However there are cases where the reduction order can have an
effect in the final result, for example there exist terms in lambda
calculus that converge when the strategy chosen is call by value
whereas they would reduced to an irreducible term when call by name is
used. 

On top of that as a user that is experimenting with a new language we
may want to trace the reduction of terms using different evaluation
orders.

Those are the reasons why NoWork needed to provide different reduction strategies and to
allow the user to choose which one will be used when a reduction happens.

\subsection{Strategies}

The simplest solution to this problem consists on predefining the
strategy to explore the tree looking for matches and coding the
rewriting algorithm to respect this strategy. This was the approach
that was implemented in the first versions of NoWork and the initial
strategies that were implemented where Bottom-up where matches are
found starting from the bottom of the tree and climbing the tree and
Top-down that does the opposite.

However the project required the implementation of more
strategies and the solution that was adopted was the definition of a
language of strategies that allows the user to specify the way the
rewriting algorithm traverses the term to rewrite it. A good
explanation of the language of strategies is provided in NoWork's user
manual. 
\section{Conclusion}

We have seen that rewriting theories have many applications not
only in computer science but also in other fields like logic and the
analysis of business processes for example. However the implementation
of rewriting theories for the verification of some of their
properties is a long process even if some of the steps could be
simplified and generalized. That is the purpose of NoWork, to simplify
the process of creating a naive version of a rewriting theory just
by describing the rewriting rules and the typing rules of the
language. Once this is done the user can test the reduction of a term
given its AST.


We have also seen a quick description of what is the process that is
carried out inside NoWork to allow the user to define a language and
to reduce a term of a rewriting theory. Then, we have explained more in depth the
implementation of the rewriting part of the process. We have
seen the importance of the evaluation order in the matches and how
NoWork started by implementing two simple hardcoded strategies
Top-Down and Bottom-Up to then provide a more powerful and flexible
solution based on a minilanguage for the description of strategies.

The resulting implementation of NoWork is a tool that allows to
define theories in a simple way and that provides a flexible language
to specify the evaluation strategy. All this features make it 
a really valuable tool for defining prototype languages
and manually verify some reduction properties on them.

From a pedagogical point of view the implementation of nominal
workbench has provided me with a deeper understanding of rewriting. 
Considering this concept in Peano Arithmetic, or CCS has widened the
vision that I had for this concept that was limited to lambda calculus
at the beginning of the project.


%\subsection{ Pedagogical value }
%What have I learned, What are the disadvantages and advantages of the
%solution proposed, how could this implementation could be improved in
%the future...

%Problem:

%Defining a framework for writing nominal rewriting systems:

%Why? Because it is useful to play with different variations of a
%rewriting systems to understand the differences in expressivity, ``puissance'',
%congruence, and normalisation properties

%(Describe every one of those properties)

%This kind of tool with the right support for traces and the right level of explanation in every
%operation can be a powerful learning tool.

%Why nominal and non-linera? The additional adjectives of non-linear and nominal 


%The initial implementation

%- Is Kleene's algorithm necessary to reduce terms to their normal form
%- Problems with rewriting systems
%  - alpha conversion + difficulty to do proofs

\end{document}
